\chapter{Conclusions}

I have created a multi-agent simulation with four different kinds of agents each of which differs in their approach to fulfill their needs. What they had to succeed in was they were put inside a two dimensional environment whereby they had to learn positions of six food resource so as to be able to survive. They were pure reactive agent, random agent, GNG agent and grid agent. Latter two had a memory to learn those positions. GNG agent used implemantion of growing neural gas, an unsupervised neural network, and grid agent used data structure inspired by \cite{Brom:placeandobjects}.

My goal was to compare those agents how good they were. These result are presented in previous chapter. [Summary of the results.] There were minor problems I had to deal with summary of which follows.

First I had to setup variables for the GNG algorithm. What I have learnt from that is the algorithm is expected to slowly converge to the learnt topology. On the other hand, I have used it for unncertain dynamicly changing data which were mostly based on other agents' believes. That is probably why much simpler memory structure used for grid agents has better results.

 