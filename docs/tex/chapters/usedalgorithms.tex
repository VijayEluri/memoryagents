\chapter{Used methods and algorithms}

In previous chapter I have introduced several kinds of agents, how they can be used and also what a spatial memory is. I have briefly prepaired you for the next chapters, where I will explain my contribution to this area. This chapter is going to cover the used algorithms and computational methods I have studied and implemented in my work. 

The first subsection dissects the implementations of agents' memory and in detail describes its fundamental parts. Both the Growing Neural Gas and the Grid are used as memory storage to handle spatial information about the environment with bounded resources.

\section{Growing Neural Gas}   
\label{usedalgo:gng}

A learning process is something what we have been experiencing every day since we were born. In this process we usually know whether we have mastered a new skill by trying it and being rewarded. Either at school, or at work. Such a procedure, whereby someone is rewarded or punished after he does something, is called a \emph{reinforcement learning} and it expects someone - a supervisor - who gives the rewards. On the other hand, sometimes we need to be able to learn without that supervisor. Growing Neural Gas (GNG) is an example of an unsupervised learning method. GNG is a self-organizing map used to find a simpler data structure representation of its origin.

\subsection{Topology learning}

Processing an enormous amount of spatial data about an environment is computationally demanding when, for example, we want to navigate in such environment. A topology learning or recognition can help us create a representation such as a topological map, which can be viewed as a graph and which makes reasoning in the environment much easier. Rather complex understanding of topology in an indoor space using Bayesian programming has been shown. \cite{Tapus:topologylearning}  

Based on competitive Hebbian learning (CHL) method \cite{Martinetz:chl} and Neural Gas (NG) \cite{Martinetz:ng}, Bern Fritzke proposed Growing Neural Gas \cite{Fritzke:gng}, an unsupervised learning method for finding a topological structure which reflects the topology of the data distribution. Although the combination of both CHL and NG is an effective method for topology learning, there are some flaws in practical application as it requires an initial set-up of number of nodes/centres that are used. This fact prevents the method from adequately describing the topology when a different number of nodes would work more effectively.

Fritzke described an algorithm that uses a set of nodes and edges that connects the nodes. A simplified description of algorithm in the context of two-dimensional space follows \cite{Fritzke:gng}:

\begin{enumerate}
\item Add two nodes at random position onto canvas.
\item Generate input signal based on the data distribution (its probability density).
\item Find the nearest node $n_1$ and second nearest node $n_2$ to the signal.
\item Increment the age of all edges leading from node $n_2$.
\item Add the squared distance between the input signal and the nearest unit in input space to a local counter variable $\Delta error(n_{1})$.
\item Move node $n_1$ and its topological neighbors towards the signal (according to parametres $epsilon_{winner}$ and $epsilon_{neighbour}$).
\item Remove all edges with an age larger than $a_{max}$.
\item Generate new nodes (see \cite{Fritzke:gng}) using variable $alpha$.
\item Decrease all error variables by multiplying them with a constant $beta$.
\item Go to 2.
\end{enumerate}

For the purpose of this work I want to use this algorithm to learn a topology of data which dynamically changes through time. We have to set-up the variables for this algorithm $alpha$, $beta$, $epsilon_{winner}$, $epsilon_{neighbour}$ and maximal number of nodes. In the following section, I am going to introduce you to the experiments carried out with this algorithm..

\subsection{Experiments on dynamic data}

As I mentioned previously, I had to set-up the variables so as to be able to use Growing Neural Gas method properly. To achieve this, I have made a Java program which tests various combinations of variables' values and finds the best one. It has subsequently run the algorithm for a given number of steps and measured the \emph{score} (see \ref{usedalgo:scoremethod} ). Throughout the experimental simulation there are randomly generated GNG inputs following a Gaussian distribution. The value of \emph{score} measures the difference between the genuin distribution and the estimated distribution which is computed based on the GNG data. The result is a combination of the distance between the distributions' centers and ratio of variances.

\begin{figure}
\begin{lstlisting}[language=Pascal]
procedure Score()
  (px, py, pvar) <- GetEstimatedGauss()
  (rx, ry, rvar) <- GetRealGauss()
  
  sqDistance <- (px - rx)*(px - rx) + (py - ry)*(py - ry)
  sqSize <- (pvar + rvar)*(pvar + rvar)
  
  score <- sqDistance / sqSize
  
  return score
end
\end{lstlisting}       
\caption{The \emph{SCORE} method}
\label{usedalgo:scoremethod}
\end{figure}
       
\begin{figure}      
\begin{center}
\includegraphics[scale=0.75]{images/gng/experimental_setup.eps}    
\caption{Screenshot showing the process of searching optimal variable values. The visualization has been made for testing purposes in the first place, but it nicely shows what had been lately paralelly computed. The bottom part of the picture shows the SCORE value throughout the simulation. {\emph (Red color means $SCORE > 0.05$, orange is $SCORE > 0.001$ and green is $SCORE <= 0.001$)} }
\end{center}                          
\label{usedalgo:gngexperimentscreen}
\end{figure}
\textbf{[\ref{usedalgo:gngexperimentscreen} proc zrovna takhle ten algoritmus, jak z toho poznam, ze tohle score je takhle ok?]}

\begin{figure}          
\begin{itemize}
\item $alpha \in \{0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0\}$
\item $beta \in \{0.0, 0.00001, 0.00005, 0.0001, 0.001, 0.005, 0.01, 0.5, 0.1, 0.5, 1.0\}$
\item $epsilon_{winner} \in \{0.0, 0.001, 0.005, 0.01, 0.1, 0.2, 0.5, 1.0\}$
\item $epsilon_{neighbour} \in \{0.0, 0.0001, 0.0006, 0.001, 0.005, 0.05, 0.1, 0.2\}$
\item $maxNodes \in \{4, 8, 16, 32\}$
\end{itemize}
\caption{Domains of variables for the experimental learning of best values.}
\label{usedalgo:gngexperimentdomains}
\end{figure}

A total number of possible combinations is equal to 19712. For each of these combinations, I ran 10000 steps of the GNG learning sequence and measured the average score. The entire experiment was computed in a parallel fashion using 30 threads.

The best results with an average $SCORE < 10^{-11}$ are shown in table \ref{usedalgo:gngexperimentresults}.

\begin{table}
\begin{center}
\begin{tabular}{ccccc|c}

$alpha$ & $beta$ & $epsilon_{winner}$ & $epsilon_{neighbour}$ & $numNodes$ & $SCORE$ \\
\hline
0.0 & 1.0 & 0.0050 & 0.0 & 16 & $3.8*10^{-12}$ \\
0.5 & 0.0 & 0.01 & 1.0E-4 & 8 & $5.1*10^{-12}$ \\   
0.5 & 0.0010 & 0.1 & 0.0010 & 8 & $8.8*10^{-12}$ \\ 
0.5 & 1.0 & 0.0 & 1.0E-4 & 8 & $3.1*10^{-12}$ \\     
0.8 & 1.0E-5 & 0.0010 & 1.0E-4 & 32 & $7.4*10^{-12}$ \\
0.8 & 1.0E-5 & 0.0050 & 6.0E-4 & 8 & $4.3*10^{-12}$ \\

\end{tabular}      
\label{usedalgo:gngexperimentresults}
\caption{Variable values with best average SCOREs}
\end{center}
\end{table}

The conclusion is that the values we have been given from this experiment and which are presented in \ref{usedalgo:gngexperimentresults} were proved to be quite unsuccessful in the main simulation. The values could not have been used, because of the bad results the GNG agents had. Therefore we have been slowly altering the values to see the immediate result in the simulation and through this way we come up with following values which works quite well.

\begin{equation} \alpha = 0.8f \\ 
\beta = 1.0E-5f \\ 
\emph{epsilon} = 5.0E-4f \\
\emph{$epsilon_2$} = 6.0E-4f \\
\emph{numNodes} = 5
\end{equation}

\section{Grid}
\label{sec:grid}

The idea for this data structure representing resource-bounded memory is based on Brom \textit{et al.} \cite{Brom:placeandobjects}. What differs in my work from their observed environment is that agents in my simulation act in a homogeneous space which cannot be differentiated in a way the mentioned simulation does. To solve this issue I have simply differentiated the environment into grid of 4x4, where each cell works as the place in \ref{subsec:howplaceandobjectscombine}.  

Each cell is given two variables \emph{positive} and \emph {negative}, both of which are set to zero and increased throughout the simulation. When an agent sees at least half of that area determined by the cell, provided he sees any food, he increases the \emph{positive} variable. If the agent searches for food and he cannot see any, he increases the \emph{negative} variable.

Later when one wants to ascertain from the grid whether there is food at specific cell, it answers according to this method with parameter {\emph a} to be found:

\begin{equation} ANSWER = \alpha\times positive - negative 
\end{equation}
 
Similarly, I will use this structure to keep the spatial information about the environment in the simulation.